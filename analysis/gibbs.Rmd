---
title: "Gibbs Sampler"
site: workflowr::wflow_site
output:
  workflowr::wflow_html:
    toc: false
editor_options:
  chunk_output_type: console
---

```{r packages, echo=FALSE}
library("DiagrammeR")
```

```{r dag, echo=FALSE}
DiagrammeR::grViz("digraph {
  graph [layout = dot, rankdir = LR]

  node [shape = circle]
  G [label = 'G']
  Z [label = 'Z']
  X [label = 'X']
  Y [label = 'Y']

  # edge definitions with the node IDs
  edge []
  G -> X [label=<\U03B8<FONT POINT-SIZE='8'><SUB>GX</SUB></FONT>>]
  G -> Y [label=<\U03B8<FONT POINT-SIZE='8'><SUB>GY</SUB></FONT>>]
  G -> Z [label=<\U03B8<FONT POINT-SIZE='8'><SUB>GZ</SUB></FONT>>]
  X -> Z [dir=back; label=<\U03B8<FONT POINT-SIZE='8'><SUB>ZX</SUB></FONT>>]
  Z -> Y [label=<\U03B8<FONT POINT-SIZE='8'><SUB>ZY</SUB></FONT>>]
  X -> Y [label=<\U03B8<FONT POINT-SIZE='8'><SUB>XY</SUB></FONT>>]
  }",
  height = 350, width = 800)
```

Continuing from the problem description on the [home page](index.html), we will use the same DAG with the SEM:

$$Z = G\theta_{GZ} + \epsilon_Z$$

$$X = G\theta_{GX} + Z\theta_{ZX} + \epsilon_X$$

$$Y = X\theta_{XY} + G\theta_{GY} + Z\theta_{ZY} + \epsilon_Y$$

Again, our goal is to estimate $\theta_{XY}$, which can be accomplished easily in a regression-based framework if we know the unknown confounding variables $\tilde{Z} = G * \theta_{GZ}$ (or only $\theta_{GZ}$ if we know $G$, or alternatively $\psi_Y = \theta_{GZ} * \theta_{ZY}$) and $\theta_{GY}$.

We explained why a sample "one-stage" factor analysis (FA) of $X$ or $\tilde{X}$ will not adequately capture $\theta_{GZ}$ -- because the latter also depends on $Y$. 
Instead, we can turn to a probabilistic framework that attempts to infer all latent parameters.


## Gibbs Sampler

The simplest approach to this problem may be to use a Gibbs sampler.
To implement this algorithm, we need to derive the distribution of each parameter conditioned on all other parameters and variables.
For simplicity, I will assume for now that all variables are normally-distributed, that all direct $G \rightarrow X$ effects are uncorrelated, $G$ are uncorrelated, $X$ do not affect each other, etc. 
Then each effect can be modeled as a univariate normal distribution. 
For notation, I will denote the effect of the $i$-th SNP on the $j$-th exposure variable as $\theta_{GX,ij}$, and likewise for all other pairs of variables. 
I will use $t$ to denote the timestep, and $t-1$ to denote the previous timestep, so we would update the preceeding variable from $\theta_{GX,ij}^{t-1} \sim \mathcal{N}(\mu_{GX}^{t-1}, \sigma_{GX}^{t-1})$ to $\theta_{GX,ij}^t \sim \mathcal{N}(\mu_{GX}^t, \sigma_{GX}^t)$.
I will use $*$ to indicate that all variables for that row or column are used, and $-j$ to indicate every variable except $j$, so for instance $\theta_{GX,*-j}$ means the effects of all SNPs on all exposures except $X_j$.

For most of the derivations to follow, I will use Bayes' Theorem to obtain probabilistic expressions (the likelihood and prior) whose distributions we know from the SEM.
Then, using conjugacy, I obtain closed-form parameter values for the posterior distribution, given the likelihood and prior from Bayes' Theorem.

I will now derive the conditional distributions of each of the latent effect sizes.
Note that I am still working on making these more polished and readable -- apologies for any sloppiness.


### Theta_GZ

The hardest effect size to infer is $\theta_{GZ}$, as its value affects both $X$ and $Y$, while all other effect sizes affect only one or the other. For $\theta_{GZ,ij}$, we can write the update step for time $t$ as

$$
\begin{align}
P(\theta_{GZ,ij}^t | G,X,Y,\theta_{-GZ}^{t-1}) 
&\propto P(Y|G,X,\theta_{GZ,ij}^t,\theta_{-GZ}^{t-1}) P(\theta_{GZ,ij}^t | G,X,\theta_{-GZ}^{t-1}) \\
&= P(Y|G,X,\theta_{GZ,ij}^t,\theta_{-GZ}^{t-1}) P(X|G,\theta_{GZ,ij}^t,\theta_{-GZ}^{t-1}) P(\theta_{GZ,ij}^t | G, \theta_{-GZ}^{t-1}) \\
&= P(Y|G,X,\theta_{GZ,ij}^t,\theta_{-GZ}^{t-1}) P(X|G,\theta_{GZ,ij}^t,\theta_{-GZ}^{t-1}) P(\theta_{GZ,ij}^t)
\end{align}
$$

We know from the SEM that

$$P(Y|G,X,\theta_{GZ,ij}^t,\theta_{-GZ}^{t-1}) = \mathcal{N}(Y; X\theta_{XY} + G\theta_{GY} + G_{-i} \theta_{GZ,-i-j}\theta_{ZY,-j} + G_i \theta_{GZ,ij}\theta_{ZY,j}, \xi_Y)$$
$$P(X|G,\theta_{GZ,ij}^t,\theta_{-GZ}^{t-1}) = \mathcal{N}(X; G\theta_{GX} + G_i\theta_{GZ,ij}\theta_{ZX,*j} + G_{-i}\theta_{GZ,-i-j}\theta_{ZX,*-j}, \xi_X)$$
Then we can write

$$P(X|G,\theta_{GZ,ij}^t,\theta_{-GZ}^{t-1}) P(\theta_{GZ,ij}^t) = \mathcal{N}(\theta_{GZ,ij}^t; \mu_{GZ}^{t*},\sigma_{GZ}^{t*})$$

where we use $t*$ to indicate that this is an intermediate computation in the update step, and

$$\sigma_{GZ}^{t*} = ([\sigma_{GZ}^{t-1}]^{-1} + [G_i\theta_{ZX,*j}]' \xi_X^{-1} [G_i\theta_{ZX,*j}])^{-1}$$

$$\mu_{GZ}^{t*} = \sigma_{GZ}^{t*} ([G_i\theta_{ZX,*j}]' \xi_X^{-1} (X - G\theta_{GX} - G_{-i}\theta_{GZ,-i-j}\theta_{ZX,*-j}) + [\sigma_{GZ}^{t-1}]^{-1} \mu_{GZ}^{t-1})$$

Using this as the prior for the likelihood $P(Y|...)$ we can now write the final update step as

$$P(\theta_{GZ,ij}^t | G,X,Y,\theta_{-GZ}^{t-1})  = \mathcal{N}(\theta_{GZ,ij}^t; \mu_{GZ}^t,\sigma_{GZ}^t)$$

where

$$\sigma_{GZ}^t = ([\sigma_{GZ}^{t*}]^{-1} + [G_i\theta_{ZY,*j}]' \xi_Y^{-1} [G_i\theta_{ZY,*j}])^{-1}$$

$$\mu_{GZ}^t = \sigma_{GZ}^t ([G_i\theta_{ZY,*j}]' \xi_Y^{-1} (Y - X \theta_{XY} - G\theta_{GY} - G_{-i}\theta_{GZ,-i-j}\theta_{ZY,*-j}) + [\sigma_{GZ}^{t*}]^{-1} \mu_{GZ}^{t*})$$


The problem I have encountered with this so far is that the dimensions don't work out -- I am looking for scalar mean and variance parameters but since a change in $\theta_{GZ}$ affects all $X$, I get multivariate parameters for $\sigma_{GZ}^{t*}$ and $\mu_{GZ}^{t*}$ when there are multiple exposures.
I am still thinking through the best way to deal with this.
One solution is that, even though I intend all $\theta_{GZ,ij}$ to be univariate normal, I may have to update them all at once -- a matrix normal update.
Or perhaps I can try to update $\psi_X$ and $\psi_Y$ instead.


### Theta_GY

For $\theta_{GY,i}$, we can write the update step for time $t$ as

$$
\begin{align}
P(\theta_{GY,i}^t | G,X,Y,\theta_{-GY}^{t-1}) 
&\propto P(Y|G,X,\theta_{GY,i}^t,\theta_{-GY}^{t-1}) P(\theta_{GY,i}^t | G,X,\theta_{-GY}^{t-1}) \\
&= P(Y|G,X,\theta_{GY,i}^t,\theta_{-GY}^{t-1}) P(\theta_{GY,i}^t) \\
&= \mathcal{N}(\theta_{GY,i}^t; \mu_{GY}^t, \sigma_{GY}^t)
\end{align}
$$
where

$$\sigma_{GY}^t = ([\sigma_{GY}^{t-1}]^{-1} + G_i' \xi_Y^{-1} G_i)^{-1}$$
$$\mu_{GY}^t = \sigma_{GY}^t (G_i' \xi_Y^{-1} (Y - X\theta_{XY} - G\theta_{GZ}\theta_{ZY} - G_{-i}\theta_{GY,-i}) + [\sigma_{GY}^{t-1}]^{-1} \mu_{GY}^{t-1})$$

### Theta_ZY

For $\theta_{ZY,i}$, we can write the update step for time $t$ as

$$
\begin{align}
P(\theta_{ZY,i}^t | G,X,Y,\theta_{-ZY}^{t-1}) 
&\propto P(Y|G,X,\theta_{ZY,i}^t,\theta_{-ZY}^{t-1}) P(\theta_{ZY,ij}^t | G,X,\theta_{-ZY}^{t-1}) \\
&= P(Y|G,X,\theta_{ZY,i}^t,\theta_{-ZY}^{t-1}) P(\theta_{ZY,i}^t) \\
&= \mathcal{N}(\theta_{ZY,i}^t; \mu_{ZY}^t, \sigma_{ZY}^t)
\end{align}
$$

where

$$\sigma_{ZY}^t = ([\sigma_{ZY}^{t-1}]^{-1} + [G_i \theta_{GZ,*i}]' \xi_Y^{-1} [G_i \theta_{GZ,*i}])^{-1}$$

$$\mu_{ZY}^t = \sigma_{ZY}^t ([G_i \theta_{GZ,*i}]' \xi_Y^{-1} (Y - X\theta_{XY} - G\theta_{GY} - G\theta_{GZ,*-i}\theta_{ZY,-i}) + [\sigma_{ZY}^{t-1}]^{-1} \mu_{ZY}^{t-1})$$


### Theta_XY

For $\theta_{XY,i}$, we can write the update step for time $t$ as

$$
\begin{align}
P(\theta_{XY,i}^t | G,X,Y,\theta_{-XY}^{t-1}) 
&\propto P(Y|G,X,\theta_{XY,i}^t,\theta_{-XY}^{t-1}) P(\theta_{XY,ij}^t | G,X,\theta_{-XY}^{t-1}) \\
&= P(Y|G,X,\theta_{XY,i}^t,\theta_{-XY}^{t-1}) P(\theta_{XY,i}^t) \\
&= \mathcal{N}(\theta_{XY,i}^t; \mu_{XY}^t, \sigma_{XY}^t)
\end{align}
$$

where

$$\sigma_{XY}^t = ([\sigma_{XY}^{t-1}]^{-1} + X_i' \xi_Y^{-1} X_i)^{-1}$$

$$\mu_{XY}^t = \sigma_{XY}^t (X_i' \xi_Y^{-1} (Y - X_{-i}\theta_{XY,-i} - G\theta_{GY} - G\theta_{GZ}\theta_{ZY}) + [\sigma_{XY}^{t-1}]^{-1} \mu_{XY}^{t-1})$$


### Theta_GX

For $\theta_{GX,ij}$, we can write the update step for time $t$ as

$$
\begin{align}
P(\theta_{GX,ij}^t | G,X,Y,\theta_{-GX}^{t-1}) 
&= P(\theta_{GX,ij}^t | G,X_j,\theta_{-GX}^{t-1}) \\
&\propto P(X_j|G,\theta_{GX,ij}^t,\theta_{-GX}^{t-1}) P(\theta_{GX,ij}^t | G,\theta_{-GX}^{t-1}) \\
&\propto P(X_j|G,\theta_{GX,ij}^t,\theta_{-GX}^{t-1}) P(\theta_{GX,ij}^t) \\
&= \mathcal{N}(\theta_{GX,ij}^t; \mu_{GX}^t, \sigma_{GX}^t)
\end{align}
$$
where

$$\sigma_{GX}^t = ([\sigma_{GX}^{t-1}]^{-1} + G_i' \xi_{X_j}^{-1} G_i)^{-1}$$

$$\mu_{GX}^t = \sigma_{GX}^t (G_i' \xi_{X_j}^{-1} (X - G\theta_{GZ}\theta_{ZX,*j} - G_{-i}\theta_{GX,-ij}) + [\sigma_{GX}^{t-1}]^{-1} \mu_{GX}^{t-1})$$


### Theta_ZX

For $\theta_{ZX,ij}$, we can write the update step for time $t$ as

$$
\begin{align}
P(\theta_{ZX,ij}^t | G,X,Y,\theta_{-ZX}^{t-1}) 
&= P(\theta_{ZX,ij}^t | G,X_j,\theta_{-ZX}^{t-1}) \\
&\propto P(X_j|G,\theta_{ZX,ij}^t,\theta_{-ZX}^{t-1}) P(\theta_{ZX,ij}^t | G,\theta_{-ZX}^{t-1}) \\
&\propto P(X_j|G,\theta_{ZX,ij}^t,\theta_{-ZX}^{t-1}) P(\theta_{ZX,ij}^t) \\
&= \mathcal{N}(\theta_{ZX,ij}^t; \mu_{ZX}^t, \sigma_{ZX}^t)
\end{align}
$$

where

$$\sigma_{ZX}^t = ([\sigma_{ZX}^{t-1}]^{-1} + [G\theta_{GZ,*i}]' \xi_{X_j}^{-1} [G\theta_{GZ,*i}])^{-1}$$

$$\mu_{ZX}^t = \sigma_{ZX}^t ([G\theta_{GZ,*i}]' \xi_{X_j}^{-1} (X - G\theta_{GZ,*-i}\theta_{ZX,-ij} - G\theta_{GX,*j}) + [\sigma_{GZ}^{t-1}]^{-1} \mu_{GZ}^{t-1})$$


## Results

The model seems to be working, because the parameter values converge to the correct values if you fix the other parameters to their true values.
The parameter estimates also stay close to the true value if you initialize them to the true values and then let them vary freely.

However, when initialized randomly and allowed to fit freely, the parameters do not always (or often) converge to the true values.
This probably means that there are multiple local optima and an incorrect local optimum is being converged towards.
I am not sure if the problem is identifiable in general without some sparsity assumptions.

I did a brief evaluation using similar settings to those on my [simulation page](simulations.html), except with only three exposures, one confounder, and ten SNPs for computational simplicity.
Parameter estimates and standard errors were evaluated by running the Gibbs sampler 100 times for each simulated dataset.
The Gibbs sampler performed similarly to vanilla two-stage least squares (2SLS), with high power but also high false positive rates.
Unlike 2SLS, the average estimate for the true beta was slightly deflated.
The average estimates for the two null variables were quite low -- close to zero (as hoped for).

Here are the raw results:

```{r gibbs-res, results='hide'}
'[nlapier2@midway3-login4 mvmr]$ Rscript eval_gibbs.R zzz/job_ 500
[1] "Average Estimates for Betas: "
[1]  0.175783706  0.010265478 -0.006009028
[1] "Standard Deviations of Betas: "
[1] 0.1787916 0.1738269 0.1816420
[1] "Average StdErrors for Betas: "
[1] 0.009712744 0.009482090 0.009499430
[1] "Positive rate for exposures: "
[1] 0.966 0.834 0.860
[1] "Bonferroni-corrected positive rate for exposures: "
[1] 0.958 0.796 0.826'
```

This gives some next steps to work on. 
First, intelligently learning priors is important -- variables are currently initialized by drawing from standard normals.
Second, sparsity is likely crucial to identifiability of the model.
Third, as discussed above, there are issues learning $\theta_{GZ}$.
Finally, it would be ideal to have flexibility beyond normally-distributed variables.
So this all points to intelligently learning ash priors via variational inference.

Also, speed is currently an issue.
This may be more specific to my current Gibbs sampler model, e.g. running many chains to get the standard error.
I implemented some momentum to help speed up slow convergence that I observed, but it's still slow.


## Next steps

* Intelligently learning priors, possibly sparse priors such as ash
* Switch to coordinate-ascent variational inference (CAVI) or some other VI
